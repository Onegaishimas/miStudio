{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec07deda",
   "metadata": {},
   "source": [
    "miStudio: End-to-End Interpretability Workflow (Live Run)\n",
    "This Jupyter notebook demonstrates the full, end-to-end workflow of the miStudio platform. We will use the four core microservices (miStudioTrain, miStudioFind, miStudioExplain, and miStudioScore) to take a base language model and produce a scored, explained set of its internal features.\n",
    "\n",
    "This is a live, non-simulated run. The steps are resource-intensive and will take a significant amount of time to complete.\n",
    "\n",
    "Workflow Overview:\n",
    "\n",
    "Train: Use miStudioTrain to train a Sparse Autoencoder (SAE) on the activations of microsoft/phi-4.\n",
    "\n",
    "Find: Use miStudioFind to analyze the trained SAE and identify interesting features from the stas/openwebtext-10k dataset.\n",
    "\n",
    "Explain: Use miStudioExplain to generate natural-language explanations for the found features.\n",
    "\n",
    "Score: Use miStudioScore to apply quantitative scores to the features based on relevance and utility.\n",
    "\n",
    "Setup: Imports and Configuration\n",
    "First, let's import the necessary Python libraries and define the URLs for our four running services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36fa0ad8",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token not found in environment variables.\n",
      "✅ Hugging Face token loaded.\n",
      "\n",
      "Service URLs configured:\n",
      "  - Train:   http://localhost:8001\n",
      "  - Find:    http://localhost:8002\n",
      "  - Explain: http://localhost:8003\n",
      "  - Score:   http://localhost:8004\n",
      "\n",
      "Data directories created at: /home/sean/app/miStudio/tests/notebook_run\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from getpass import getpass\n",
    "\n",
    "# --- Service Endpoints ---\n",
    "# These should match the ports you started the services on.\n",
    "BASE_URL = \"http://localhost\"\n",
    "TRAIN_URL = f\"{BASE_URL}:8001\"\n",
    "FIND_URL = f\"{BASE_URL}:8002\"\n",
    "EXPLAIN_URL = f\"{BASE_URL}:8003\"\n",
    "SCORE_URL = f\"{BASE_URL}:8004\"\n",
    "\n",
    "# --- Shared Data Paths ---\n",
    "# We will assume a shared volume or directory accessible by all services.\n",
    "# For this demo, we'll place inputs in a 'notebook_run' directory.\n",
    "DATA_ROOT = \"./notebook_run\"\n",
    "INPUT_DIR = os.path.join(DATA_ROOT, \"input\")\n",
    "OUTPUT_DIR = os.path.join(DATA_ROOT, \"output\")\n",
    "CONFIG_DIR = os.path.join(DATA_ROOT, \"config\")\n",
    "\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "\n",
    "# --- Hugging Face Token ---\n",
    "# Required for accessing certain models like phi-4.\n",
    "# Best practice is to load from an environment variable.\n",
    "hf_token = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "if not hf_token:\n",
    "    print(\"Hugging Face token not found in environment variables.\")\n",
    "    hf_token = getpass(\"Please enter your Hugging Face token: \")\n",
    "\n",
    "if not hf_token:\n",
    "    print(\"⚠️ Warning: No Hugging Face token provided. The 'train' step may fail if the model is gated.\")\n",
    "else:\n",
    "    print(\"✅ Hugging Face token loaded.\")\n",
    "\n",
    "\n",
    "# --- Global Variables ---\n",
    "# To store the output paths from each step\n",
    "sae_model_path = None\n",
    "features_path = None\n",
    "explanations_path = None\n",
    "scores_path = None\n",
    "\n",
    "print(f\"\\nService URLs configured:\")\n",
    "print(f\"  - Train:   {TRAIN_URL}\")\n",
    "print(f\"  - Find:    {FIND_URL}\")\n",
    "print(f\"  - Explain: {EXPLAIN_URL}\")\n",
    "print(f\"  - Score:   {SCORE_URL}\")\n",
    "print(f\"\\nData directories created at: {os.path.abspath(DATA_ROOT)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f27a1",
   "metadata": {},
   "source": [
    "Step 1: Train an SAE with miStudioTrain\n",
    "This is the first and most time-consuming step. We will send a request to the miStudioTrain service to train an SAE on the activations of a mid-level layer from the Phi-4 model.\n",
    "\n",
    "WARNING: This cell will run for a very long time (potentially hours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523e1b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Step 1: Train\n",
      "This step will take a long time. Please be patient.\n",
      "\n",
      "Sending training request to http://localhost:8001/train_model/ with payload:\n",
      "{\n",
      "  \"model_name\": \"microsoft/phi-4\",\n",
      "  \"dataset_name\": \"stas/openwebtext-10k\",\n",
      "  \"layer\": \"model.layers.24.mlp\",\n",
      "  \"output_dir\": \"./notebook_run/output\",\n",
      "  \"hf_token\": \"hf_...[REDACTED]\"\n",
      "}\n",
      "❌ Error during Train step: 404 Client Error: Not Found for url: http://localhost:8001/train_model/\n"
     ]
    }
   ],
   "source": [
    "print(\"▶️ Step 1: Train\")\n",
    "print(\"This step will take a long time. Please be patient.\")\n",
    "\n",
    "# Define the parameters for our training job\n",
    "MODEL_NAME = \"microsoft/phi-4\" # Updated model\n",
    "DATASET_NAME = \"stas/openwebtext-10k\"\n",
    "# NOTE: This layer is a guess for a larger model. You may need to update it.\n",
    "TARGET_LAYER = \"model.layers.24.mlp\" \n",
    "\n",
    "train_payload = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"dataset_name\": DATASET_NAME,\n",
    "    \"layer\": TARGET_LAYER,\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"hf_token\": hf_token # Pass the token to the service\n",
    "}\n",
    "\n",
    "print(f\"\\nSending training request to {TRAIN_URL}/train_model/ with payload:\")\n",
    "# Create a copy of the payload to print, but hide the token\n",
    "printable_payload = train_payload.copy()\n",
    "printable_payload[\"hf_token\"] = \"hf_...[REDACTED]\"\n",
    "print(json.dumps(printable_payload, indent=2))\n",
    "\n",
    "try:\n",
    "    # We use a very long timeout because this is a blocking, long-running task.\n",
    "    # CORRECTED a typo in the URL from /train to /train_sae/\n",
    "    response = requests.post(f\"{TRAIN_URL}/train_model/\", json=train_payload, timeout=10800) # 3 hour timeout\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    train_result = response.json()\n",
    "    # We assume the service returns a path to the trained model.\n",
    "    # The key is assumed to be 'output_path' based on other services.\n",
    "    sae_model_path = train_result.get(\"output_path\")\n",
    "    \n",
    "    if sae_model_path:\n",
    "        print(f\"\\n✅ Success! Training complete. SAE model saved to: {sae_model_path}\")\n",
    "    else:\n",
    "        print(f\"❌ Error: Training job finished but did not return an output path.\")\n",
    "        print(\"Response from server:\", train_result)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"❌ Error during Train step: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604a838",
   "metadata": {},
   "source": [
    "Step 2: Find Interesting Features with miStudioFind\n",
    "Now that we have a trained SAE, we'll use the miStudioFind service to analyze it and extract a list of features with their statistics and top activating examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f936849",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sae_model_path:\n",
    "    print(\"\\n▶️ Step 2: Find\")\n",
    "\n",
    "    find_payload = {\n",
    "        \"sae_model_path\": sae_model_path,\n",
    "        \"dataset_name\": DATASET_NAME,\n",
    "        \"output_dir\": OUTPUT_DIR\n",
    "    }\n",
    "\n",
    "    print(f\"Sending request to {FIND_URL}/find...\")\n",
    "    try:\n",
    "        response = requests.post(f\"{FIND_URL}/find\", json=find_payload, timeout=10800) # 3 hour timeout\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        find_result = response.json()\n",
    "        features_path = find_result.get(\"output_path\")\n",
    "        \n",
    "        print(f\"✅ Success! Features saved to: {features_path}\")\n",
    "\n",
    "        features_df = pd.read_json(features_path)\n",
    "        print(\"\\nSample of found features:\")\n",
    "        display(features_df.head())\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Error during Find step: {e}\")\n",
    "        features_path = None\n",
    "else:\n",
    "    print(\"\\nSkipping Step 2: Find, as the previous step failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a0566",
   "metadata": {},
   "source": [
    "Step 3: Explain Features with miStudioExplain\n",
    "With our list of features, we can now use the miStudioExplain service to generate human-readable explanations for what each feature represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_path:\n",
    "    print(\"\\n▶️ Step 3: Explain\")\n",
    "    \n",
    "    explain_payload = {\n",
    "        \"feature_data_path\": features_path,\n",
    "        \"output_dir\": OUTPUT_DIR\n",
    "    }\n",
    "\n",
    "    print(f\"Sending request to {EXPLAIN_URL}/explain...\")\n",
    "    try:\n",
    "        response = requests.post(f\"{EXPLAIN_URL}/explain\", json=explain_payload, timeout=3600) # 1 hour timeout\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        explain_result = response.json()\n",
    "        explanations_path = explain_result.get(\"output_path\")\n",
    "        \n",
    "        print(f\"✅ Success! Explanations saved to: {explanations_path}\")\n",
    "\n",
    "        explanations_df = pd.read_json(explanations_path)\n",
    "        print(\"\\nSample of explained features:\")\n",
    "        display(explanations_df[[\"feature_index\", \"explanation\"]].head())\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Error during Explain step: {e}\")\n",
    "        explanations_path = None\n",
    "else:\n",
    "    print(\"\\nSkipping Step 3: Explain, as the previous step failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe7749",
   "metadata": {},
   "source": [
    "Step 4: Score Features with miStudioScore\n",
    "This is the final step. We will use the miStudioScore service to apply both a relevance score and a more advanced ablation-based utility score.\n",
    "\n",
    "First, we need to create the scoring_config.yaml and a dummy benchmark_dataset.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the benchmark dataset file for ablation scoring\n",
    "benchmark_content = \"\"\"\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_benchmark(model, tokenizer, device):\n",
    "    # A simple dummy benchmark that calculates perplexity on a fixed sentence.\n",
    "    # A real benchmark would use a proper evaluation dataset.\n",
    "    text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings, labels=encodings.input_ids)\n",
    "        loss = outputs.loss\n",
    "    \n",
    "    return loss.item()\n",
    "\"\"\"\n",
    "benchmark_path = os.path.join(CONFIG_DIR, \"notebook_benchmark.py\")\n",
    "with open(benchmark_path, \"w\") as f:\n",
    "    f.write(benchmark_content)\n",
    "print(f\"Benchmark script created at: {benchmark_path}\")\n",
    "\n",
    "\n",
    "# Create the scoring configuration file dynamically for this run\n",
    "scoring_config = {\n",
    "    \"scoring_jobs\": [\n",
    "        {\n",
    "            \"scorer\": \"relevance_scorer\",\n",
    "            \"name\": \"code_relevance\",\n",
    "            \"params\": {\n",
    "                \"positive_keywords\": [\"python\", \"def\", \"import\", \"class\", \"return\", \"for\", \"while\"],\n",
    "                \"negative_keywords\": [\"marketing\", \"recipe\", \"sports\", \"weather\"],\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"scorer\": \"ablation_scorer\",\n",
    "            \"name\": \"perplexity_utility\",\n",
    "            \"params\": {\n",
    "                \"benchmark_dataset_path\": benchmark_path,\n",
    "                \"target_model_name\": MODEL_NAME,\n",
    "                \"target_model_layer\": TARGET_LAYER,\n",
    "                \"device\": \"cuda\", # Change to \"cpu\" if not using a GPU\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "scoring_config_path = os.path.join(CONFIG_DIR, \"notebook_scoring_config.yaml\")\n",
    "with open(scoring_config_path, \"w\") as f:\n",
    "    yaml.dump(scoring_config, f, indent=2)\n",
    "\n",
    "print(f\"Scoring configuration created at: {scoring_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a67f17",
   "metadata": {},
   "source": [
    "Now we can call the scoring service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e7dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if explanations_path:\n",
    "    print(\"\\n▶️ Step 4: Score\")\n",
    "    \n",
    "    score_payload = {\n",
    "        \"features_path\": explanations_path, # Use the output from the previous step\n",
    "        \"config_path\": scoring_config_path,\n",
    "        \"output_dir\": OUTPUT_DIR\n",
    "    }\n",
    "\n",
    "    print(f\"Sending request to {SCORE_URL}/score...\")\n",
    "    try:\n",
    "        response = requests.post(f\"{SCORE_URL}/score\", json=score_payload, timeout=10800) # 3 hour timeout for ablation\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        score_result = response.json()\n",
    "        scores_path = score_result.get(\"output_path\")\n",
    "        \n",
    "        print(f\"✅ Success! Final scores saved to: {scores_path}\")\n",
    "\n",
    "        scores_df = pd.read_json(scores_path)\n",
    "        print(\"\\nFinal scored and explained features (sorted by code relevance):\")\n",
    "        display(scores_df.sort_values(by=\"code_relevance\", ascending=False).head(10))\n",
    "        \n",
    "        print(\"\\nFinal scored and explained features (sorted by perplexity utility):\")\n",
    "        # A higher utility score (more negative impact when removed) means the feature is more important\n",
    "        display(scores_df.sort_values(by=\"perplexity_utility\", ascending=False).head(10))\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"❌ Error during Score step: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping Step 4: Score, as the previous step failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6375aa",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "This notebook has successfully demonstrated the full miStudio pipeline. We have shown how a series of independent microservices can be chained together to create a powerful and flexible interpretability workflow, taking a large language model and turning its opaque internal workings into scored, explained, and actionable insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miStudio-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
