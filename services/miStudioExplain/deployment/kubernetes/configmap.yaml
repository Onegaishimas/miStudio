apiVersion: v1
kind: ConfigMap
metadata:
  name: mistudio-explain-config
  namespace: mistudio-services
  labels:
    app: mistudio-explain
    component: configuration
data:
  service.yaml: |
    ollama:
      service_name: "ollama"
      namespace: "mistudio-services"
      port: 11434
      timeout: 300
      retry_attempts: 3
    gpu:
      rtx_3090_device: "cuda:0"
      rtx_3080_ti_device: "cuda:1"
      memory_buffer_mb: 1024
      max_utilization_percent: 90.0
    processing:
      default_quality_threshold: 0.4
      max_concurrent_jobs: 3
      default_batch_size: 10
      explanation_timeout: 120
      max_retries: 2
    log_level: "INFO"
    data_path: "/app/data"
    cache_path: "/app/data/cache"
  models.yaml: |
    models:
      llama3.1:8b:
        gpu_memory_mb: 8192
        target_gpu: "RTX_3080_Ti"
        use_cases: ["simple_patterns", "quick_explanations"]
        max_concurrent: 2
        parameters:
          temperature: 0.1
          top_p: 0.9
          max_tokens: 200
      llama3.1:70b:
        gpu_memory_mb: 20480
        target_gpu: "RTX_3090"
        use_cases: ["complex_behavioral", "detailed_analysis"]
        max_concurrent: 1
        parameters:
          temperature: 0.1
          top_p: 0.9
          max_tokens: 300
      codellama:13b:
        gpu_memory_mb: 12288
        target_gpu: "RTX_3080_Ti"
        use_cases: ["technical_patterns", "code_analysis"]
        max_concurrent: 1
        parameters:
          temperature: 0.0
          top_p: 0.95
          max_tokens: 250
      mistral:7b:
        gpu_memory_mb: 7168
        target_gpu: "RTX_3080_Ti"
        use_cases: ["fallback", "lightweight_processing"]
        max_concurrent: 2
        parameters:
          temperature: 0.2
          top_p: 0.9
          max_tokens: 200

