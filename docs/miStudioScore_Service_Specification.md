miStudioScore: Service Specification (As-Built)
1. Service Purpose & Goal üéØ
The miStudioScore service is a critical component of the advanced analysis pipeline, designed to move beyond the qualitative analysis of "what a feature represents" and provide a quantitative, data-driven answer to the question, "how important is this feature?"

Its primary goal is to ingest the feature data generated by miStudioFind and enrich it with one or more numerical scores. These scores allow users to rank, filter, and prioritize features based on their causal utility for specific tasks, their safety implications, or their relevance to custom business objectives. By providing a concrete importance metric, this service enables a more focused and efficient interpretability workflow.

2. Inputs üìÅ
The service is configuration-driven, operating on three key file inputs:

features.json (Required): The primary data input, generated by miStudioFind. It contains a list of feature objects, each expected to have a feature_index and top_activating_examples.

scoring_config.yaml (Required): The "instruction manual" for a scoring run, located in the /config directory. It defines which scoring methods to apply and specifies their parameters.

benchmark_dataset.py (Optional): A user-provided Python file required for "Ablation Scoring." It must contain a run_benchmark(model, tokenizer, device) function that returns a single float value representing model performance.

3. Workflow & Service Logic ‚öôÔ∏è
The service follows an orchestrated workflow managed by the ScoringOrchestrator.

Step 1: Initialization & Validation

The FastAPI application in src/main.py receives a request at the /score endpoint.

An instance of ScoringOrchestrator (from src/core/orchestrator.py) is created, which loads and validates the scoring_config.yaml.

The orchestrator dynamically discovers and loads available scorer modules (e.g., RelevanceScorer, AblationScorer) from the src/scorers/ directory.

Step 2: Score Calculation Loop

The orchestrator's run method is executed. It loads the features.json file.

It iterates through the scoring_jobs defined in the configuration. For each job, it matches the specified scorer name to a loaded scorer module.

Method A: Relevance Scoring (RelevanceScorer)

It analyzes the top_activating_examples for each feature against positive_keywords and negative_keywords from the job's parameters.

It calculates and appends a normalized score to each feature object under the name specified in the job config.

Method B: Ablation Scoring (AblationScorer)

It loads the specified Hugging Face model and the trained SAE model using helpers from src/utils/model_loader.py.

It dynamically loads the run_benchmark function from the user-provided benchmark_dataset.py.

It calculates a baseline performance score by running the benchmark on the unmodified model.

It then iterates through each feature, registering a forward hook on the target model layer to ablate one feature at a time.

For each ablated feature, it re-runs the entire benchmark. The utility score (the difference between the ablated score and the baseline) is calculated and appended to the feature object.

Step 3: Output Generation

After all scoring jobs are complete, the orchestrator saves the enriched list of feature objects to a new, timestamped scores_...json file in the specified output directory.

The API endpoint returns a success response containing the path to this new file.

4. Modular Design & File Structure (As-Built) üß±
The final implementation follows the project's standard conventions, with all source code located under the src/ directory.

src/main.py (API Layer): The FastAPI application entry point. Defines the /score and /health endpoints.

src/core/orchestrator.py (Workflow Layer): Contains the ScoringOrchestrator class, which manages the overall workflow, configuration parsing, and dynamic loading of scorer modules.

src/models/ (Data Models):

api_models.py: Pydantic models (ScoreRequest, ScoreResponse) for API validation.

sae_model.py: A PyTorch nn.Module defining the SparseAutoencoder class structure, required for loading trained SAEs.

src/scorers/ (Scoring Logic): The plug-and-play directory for all scoring algorithms.

base_scorer.py: Defines the abstract BaseScorer class that all other scorers inherit from.

relevance_scorer.py: Implements the keyword-based relevance scoring logic.

ablation_scorer.py: Implements the causal utility scoring logic using model hooks and benchmarking.

src/utils/ (Utilities):

file_handler.py: Helper functions for reading/writing JSON and YAML files.

model_loader.py: Logic for loading Hugging Face models and custom SAE PyTorch models.

logging_config.py: Standardized application logging setup.

5. Development Backlog Status ‚úÖ
The initial development specified in the backlog is complete.

Sprint 1: Core Functionality & Relevance Scoring: Complete.

Sprint 2: Ablation Scoring: Complete.

Sprint 3: Optimization & Finalization: Ready to begin. Key tasks include optimizing the ablation process, enhancing error handling, and finalizing deployment artifacts.