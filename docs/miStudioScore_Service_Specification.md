________________________________________
miStudioScore Service Specification
1. Service Purpose & Goal üéØ
The miStudioScore service is designed to move beyond the qualitative analysis of "what a feature represents" and provide a quantitative, data-driven answer to the question, "how important is this feature?"
Its primary goal is to ingest the feature data generated by miStudioFind and enrich it with one or more numerical scores. These scores will allow users to rank, filter, and prioritize features based on their utility, safety implications, or relevance to specific business objectives. By providing a concrete importance metric, this service enables a more focused and efficient interpretability workflow, allowing researchers and developers to concentrate on the features that matter most.
2. Inputs üìÅ
The service is designed to operate on three key input files:
1.	features.json (Required): This is the primary input file, generated by the miStudioFind service. It contains a list of all features extracted from the Sparse Autoencoder (SAE), along with their statistical properties and top activating examples. The scoring service will read this file, append new scoring data to each feature object, and write a new, enriched version.
2.	scoring_config.json (Required): This file defines which scoring methods to apply and specifies their parameters. This configuration-driven approach makes the service highly flexible, as users can define custom scoring runs without changing the service's code. It tells the service what to do.
3.	benchmark_dataset.py (Optional): This Python file is required only when performing "Ablation Scoring." It contains the logic for a specific performance benchmark (e.g., measuring the model's accuracy on a Q&A task or its loss on a summarization task). This file allows users to measure a feature's utility against a task that is directly relevant to them.
3. Workflow & Service Logic ‚öôÔ∏è
The service will follow a clear, orchestrated workflow upon receiving a request:
Step 1: Initialization & Validation
‚Ä¢	The service starts and receives a request containing paths to the three input files.
‚Ä¢	It loads the scoring_config.json and validates its structure.
‚Ä¢	It loads the main features.json file into memory.
Step 2: Score Calculation Loop
‚Ä¢	The service iterates through the list of scoring methods defined in the scoring_config.json. For each method, it performs the following:
o	Method A: Business Relevance Scoring (Keyword Correlation)
ÔÇß	If the config specifies type: "relevance", the service reads the provided lists of keywords (e.g., positive_keywords, negative_keywords).
ÔÇß	For each feature in features.json, it analyzes the text of its top activating examples.
ÔÇß	It calculates a score based on the frequency and presence of the specified keywords. For example, a feature whose activating examples consistently contain words from the positive_keywords list will receive a high positive score.
ÔÇß	The resulting score (e.g., relevance_score: 0.85) is appended to the feature's data.
o	Method B: Ablation Scoring (Causal Utility)
ÔÇß	If the config specifies type: "ablation", this triggers the most complex scoring path.
ÔÇß	The service loads the base LLM and the trained SAE model into memory.
ÔÇß	It dynamically imports the benchmark_dataset.py file to get the evaluation logic.
ÔÇß	First, it runs the benchmark on the unmodified model to establish a baseline performance score (e.g., baseline_loss: 1.25).
ÔÇß	Then, it iterates through each feature to be scored. For each feature, it performs a "causal intervention":
1.	It attaches a forward hook to the base model's target layer.
2.	This hook is designed to "ablate" (zero out) the specific feature's contribution to the activation vector.
3.	It runs the entire benchmark again with the feature-ablated model.
4.	The new, "ablated" performance score is recorded. The difference between the baseline score and the ablated score is the feature's utility score. A large increase in loss indicates a highly useful feature for that task.
ÔÇß	This utility score (e.g., utility_score_qa_benchmark: 0.15) is appended to the feature's data.
Step 3: Output Generation
‚Ä¢	After all scoring methods in the configuration have been executed, the service saves the modified list of feature objects to a new output file, scores.json.
‚Ä¢	The service then terminates the request, returning the path to the scores.json file.
4. Modular Design & Backlog üß±
To ensure the service is maintainable, scalable, and easy to develop, it will be designed with a modular structure. This structure is also ideal for containerization, as dependencies are well-defined.
Modular Components:
‚Ä¢	main.py (API Layer): A thin FastAPI application that defines the /score endpoint. Its only job is to handle HTTP requests, parse the input, and delegate the core logic to the ScoringOrchestrator.
‚Ä¢	orchestrator.py (Workflow Layer): The ScoringOrchestrator class. This is the heart of the service. It reads the config file and calls the appropriate scoring modules in sequence. It manages the overall workflow.
‚Ä¢	scorers/ (Module Directory): A directory containing individual scoring modules. This plug-and-play design allows new scoring methods to be added easily in the future.
o	scorers/base_scorer.py: An abstract base class that defines the interface for all scorers (e.g., a score() method).
o	scorers/relevance_scorer.py: The implementation for Business Relevance Scoring.
o	scorers/ablation_scorer.py: The implementation for Ablation Scoring. This module will handle the complex logic of model loading, patching, and benchmarking.
‚Ä¢	utils/ (Utility Directory):
o	utils/file_handler.py: Helper functions for reading and writing JSON files.
o	utils/model_loader.py: Logic for loading Hugging Face models and SAEs, ensuring they are loaded onto the correct device (CPU/GPU).
Development Backlog:
This backlog breaks down the creation of the service into manageable tasks.
Sprint 1: Core Functionality & Relevance Scoring
1.	Task-1: Set up the basic FastAPI application structure in main.py.
2.	Task-2: Implement the ScoringOrchestrator in orchestrator.py with initial logic to parse the config file.
3.	Task-3: Create the file_handler.py utility.
4.	Task-4: Implement the RelevanceScorer module. This is the simplest scorer and provides a good baseline.
5.	Task-5: Write unit tests for the relevance scorer and orchestrator.
6.	Task-6: Create the initial Dockerfile for containerizing the service.
Sprint 2: Ablation Scoring
1.	Task-7: Implement the model_loader.py utility to handle loading both the base LLM and the SAE.
2.	Task-8: Implement the core logic for the AblationScorer, focusing on the model hooking and patching mechanism.
3.	Task-9: Implement the dynamic loading of the benchmark_dataset.py file.
4.	Task-10: Add GPU support and resource management for the ablation process.
5.	Task-11: Write comprehensive integration tests for the full ablation workflow.
Sprint 3: Optimization & Finalization
1.	Task-12: Optimize the ablation process. Investigate if multiple features can be scored in a single pass to reduce redundant benchmark runs.
2.	Task-13: Add robust error handling and logging throughout the service.
3.	Task-14: Create detailed documentation for the scoring_config.json format and how to write a benchmark_dataset.py file.
4.	Task-15: Finalize the Kubernetes deployment manifests (Deployment, Service, ConfigMap) for the service.

